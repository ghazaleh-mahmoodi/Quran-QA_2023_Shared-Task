{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 52.7 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.0-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.1/78.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.24.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 65.9 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.1-cp310-cp310-win_amd64.whl (44.0 MB)\n",
      "     --------------------------------------- 44.0/44.0 MB 54.4 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 93.8 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ------------------------------------- 977.5/977.5 kB 60.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     ------------------------------------- 268.8/268.8 kB 16.2 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Collecting packaging>=20.9\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "     -------------------------------------- 163.8/163.8 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.0)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp310-cp310-win_amd64.whl (268 kB)\n",
      "     ---------------------------------------- 268.3/268.3 kB ? eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 73.6 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.2-cp310-cp310-win_amd64.whl (266 kB)\n",
      "     ---------------------------------------- 266.3/266.3 kB ? eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 302.2/302.2 kB ? eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n",
      "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, threadpoolctl, scipy, regex, pyyaml, packaging, joblib, fsspec, colorama, tqdm, scikit-learn, click, nltk, huggingface-hub, transformers, sentence-transformers\n",
      "  Running setup.py install for sentence-transformers: started\n",
      "  Running setup.py install for sentence-transformers: finished with status 'done'\n",
      "Successfully installed click-8.1.6 colorama-0.4.6 fsspec-2023.6.0 huggingface-hub-0.16.4 joblib-1.3.2 nltk-3.8.1 packaging-23.1 pyyaml-6.0.1 regex-2023.8.8 safetensors-0.3.2 scikit-learn-1.3.0 scipy-1.11.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 threadpoolctl-3.2.0 tokenizers-0.13.3 tqdm-4.66.0 transformers-4.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\gargo\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrec_eval\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Using legacy 'setup.py install' for pytrec_eval, since package 'wheel' is not installed.\n",
      "Installing collected packages: pytrec_eval\n",
      "  Running setup.py install for pytrec_eval: started\n",
      "  Running setup.py install for pytrec_eval: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for pytrec_eval did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  Fetching trec_eval from https://github.com/usnistgov/trec_eval/archive/v9.0.8.tar.gz.\n",
      "  C:\\Users\\gargo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running install\n",
      "  C:\\Users\\gargo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  copying py\\__init__.py -> build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  running build_ext\n",
      "  building 'pytrec_eval_ext' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "pytrec_eval\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\gargo\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Arabic-Stopwords in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: pyarabic>=0.6.2 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Arabic-Stopwords) (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\gargo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyarabic>=0.6.2->Arabic-Stopwords) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\gargo\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from snowballstemmer import stemmer\n",
    "import arabicstopwords.arabicstopwords as ar_stp\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pyterrier as pt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "index_path = os.path.join(data_path, \"QPC_Index/data.properties\")\n",
    "\n",
    "query_train_path = os.path.join(data_path, \"QQA23_TaskA_train.tsv\")\n",
    "query_dev_path = os.path.join(data_path, \"QQA23_TaskA_dev.tsv\")\n",
    "\n",
    "passage_path = os.path.join(data_path, \"Thematic_QPC/QQA23_TaskA_QPC_v1.1.tsv\")\n",
    "\n",
    "qp_pair_train_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_train.gold\")\n",
    "qp_pair_dev_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_dev.gold\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# read file based on its extension (tsv or xlsx)\n",
    "def read_file(input_file, sep=\"\\t\", names = \"\"):\n",
    "    if input_file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(input_file)\n",
    "    else:\n",
    "        if names != \"\":\n",
    "            df = pd.read_csv(input_file, sep=sep, names=names,encoding=\"utf-8\")\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, sep=sep,encoding=\"utf-8\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:54.429078600Z",
     "start_time": "2023-08-09T20:37:54.412948400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Query-Passage Pair"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "qrels_columns = [\"qid\", \"Q0\", \"docid\", \"relevance\"]\n",
    "\n",
    "def read_qrels_file(qrels_file):\n",
    "    # split_token = '\\t' if format_checker.is_tab_sparated(qrels_file) else  \"\\s+\"\n",
    "    df_qrels = pd.read_csv(qrels_file, sep='\\t', names=qrels_columns)\n",
    "    df_qrels[\"qid\"] = df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docid\"] = df_qrels[\"docid\"].astype(str)\n",
    "    return df_qrels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:55.062630400Z",
     "start_time": "2023-08-09T20:37:55.046607Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_index(index_path):\n",
    "    if not pt.started():\n",
    "        pt.init(helper_version=\"0.0.6\")\n",
    "\n",
    "    try:\n",
    "        index = pt.IndexFactory.of(index_path)\n",
    "        print(\"Index was loaded successfully from this path: \", index_path)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print('Cannot load the index, check exception details {}'.format(e))\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:55.487421800Z",
     "start_time": "2023-08-09T20:37:55.470725800Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Clean text from urls, handles, special characters, tabs, line jumps, extra white space, and puntuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:55.875674500Z",
     "start_time": "2023-08-09T20:37:55.859637300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean text from urls, handles, special characters, tabs, line jumps, and extra white space.\n",
    "def clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "    text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "    text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
    "    text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Removing punctuations in string using regex\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocess the arabic input text by performing normalization, stemming, and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.239976800Z",
     "start_time": "2023-08-09T20:37:56.176683800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# arabic stemmer\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m ar_stemmer \u001B[38;5;241m=\u001B[39m \u001B[43mstemmer\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marabic\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# remove arabic stop words\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mar_remove_stop_words\u001B[39m(sentence):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# arabic stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "\n",
    "# remove arabic stop words\n",
    "def ar_remove_stop_words(sentence):\n",
    "    terms=[]\n",
    "    stopWords= set(ar_stp.stopwords_list())\n",
    "    for term in sentence.split() : \n",
    "        if term not in stopWords :\n",
    "            terms.append(term)\n",
    "    return \" \".join(terms)\n",
    "\n",
    "\n",
    "# normalize the arabic text\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return(text)\n",
    "\n",
    "# stem the arabic text\n",
    "def ar_stem(sentence):\n",
    "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
    "\n",
    "\n",
    "# apply all preprocessing steps needed for Arabic text\n",
    "def preprocess_arabic(text): \n",
    "    text = normalize_arabic(text)\n",
    "    text = ar_remove_stop_words(text)\n",
    "    text = ar_stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def prepare_data(path, column, id_type, id_column='docno'):\n",
    "        df = read_file(path, names=['docno', 'text'])\n",
    "\n",
    "        print(\"Cleaning passages\")\n",
    "        # apply the cleaning functions on the queries/questions\n",
    "        df[column] = df['text'].apply(clean)\n",
    "\n",
    "        # apply normalization, stemming and stop word removal\n",
    "        print(\"Preprocessing - Applying normalization, stemming and stop word removal\")\n",
    "        df[column] = df[column].apply(preprocess_arabic)\n",
    "\n",
    "        df[id_type] = df[id_column].astype(str) # convert the id column to string\n",
    "        df = df[[id_type, 'text', column]] # keep the columns needed for search\n",
    "\n",
    "        print(\"Done with preparation!\")\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.311095100Z",
     "start_time": "2023-08-09T20:37:56.291456900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m index \u001B[38;5;241m=\u001B[39m load_index(index_path\u001B[38;5;241m=\u001B[39m\u001B[43mindex_path\u001B[49m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(index\u001B[38;5;241m.\u001B[39mgetCollectionStatistics()\u001B[38;5;241m.\u001B[39mtoString())\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(index\u001B[38;5;241m.\u001B[39mgetMetaIndex()\u001B[38;5;241m.\u001B[39mgetKeys())\n",
      "\u001B[1;31mNameError\u001B[0m: name 'index_path' is not defined"
     ]
    }
   ],
   "source": [
    "index = load_index(index_path=index_path)\n",
    "\n",
    "print(index.getCollectionStatistics().toString())\n",
    "print(index.getMetaIndex().getKeys())\n",
    "\n",
    "# for kv in index.getLexicon():\n",
    "#     print((kv.getKey())+\"\\t\"+ kv.getValue().toString())\n",
    "# index.getLexicon()[\"فاعل\"].toString()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.663848700Z",
     "start_time": "2023-08-09T20:37:56.642766300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Passage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'passage_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_passage \u001B[38;5;241m=\u001B[39m prepare_data(\u001B[43mpassage_path\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpassage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m df_passage\n",
      "\u001B[1;31mNameError\u001B[0m: name 'passage_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_passage = prepare_data(passage_path, 'passage', 'pid')\n",
    "df_passage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.748220100Z",
     "start_time": "2023-08-09T20:37:56.730849100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_query_train \u001B[38;5;241m=\u001B[39m prepare_data(\u001B[43mquery_train_path\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m df_query_train\n",
      "\u001B[1;31mNameError\u001B[0m: name 'query_train_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_query_train = prepare_data(query_train_path, 'query', 'qid')\n",
    "df_query_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.842028300Z",
     "start_time": "2023-08-09T20:37:56.823004600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_dev_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_query_dev \u001B[38;5;241m=\u001B[39m prepare_data(\u001B[43mquery_dev_path\u001B[49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m df_query_dev\n",
      "\u001B[1;31mNameError\u001B[0m: name 'query_dev_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_query_dev = prepare_data(query_dev_path, 'query', 'qid')\n",
    "df_query_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:56.904825400Z",
     "start_time": "2023-08-09T20:37:56.855032200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query-Passage Pair"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qp_pair_train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_qppair_train \u001B[38;5;241m=\u001B[39m read_qrels_file(\u001B[43mqp_pair_train_path\u001B[49m)\n\u001B[0;32m      2\u001B[0m df_qppair_train\n",
      "\u001B[1;31mNameError\u001B[0m: name 'qp_pair_train_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_qppair_train = read_qrels_file(qp_pair_train_path)\n",
    "df_qppair_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T20:37:57.013634900Z",
     "start_time": "2023-08-09T20:37:56.935852800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qp_pair_dev_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_qppair_dev \u001B[38;5;241m=\u001B[39m read_qrels_file(\u001B[43mqp_pair_dev_path\u001B[49m)\n\u001B[0;32m      2\u001B[0m df_qppair_dev\n",
      "\u001B[1;31mNameError\u001B[0m: name 'qp_pair_dev_path' is not defined"
     ]
    }
   ],
   "source": [
    "df_qppair_dev = read_qrels_file(qp_pair_dev_path)\n",
    "df_qppair_dev"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T20:37:56.950357200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Embedding\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from simcse import SimSCE\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, CrossEncoder, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T01:38:09.402247400Z",
     "start_time": "2023-08-09T01:38:08.593713300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T01:38:09.417511400Z",
     "start_time": "2023-08-09T01:38:09.403251500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Retrieval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def save_query_passage_retrieval(result, tag, run_save=False):\n",
    "    if tag == \"BM25\":\n",
    "        result[\"Q0\"] = [\"Q0\"] * len(result)\n",
    "        result[\"tag\"] = [tag] * len(result)\n",
    "        result['qid'] = result[\"qid\"]\n",
    "        result['pid'] = result[\"docno\"]\n",
    "        result = result[[\"qid\", \"Q0\", \"pid\", \"rank\", \"score\", \"tag\"]]\n",
    "\n",
    "    elif tag == \"SimCSE_biencoder\":\n",
    "        np_result = np.array(result).flatten()\n",
    "        result = pd.DataFrame()\n",
    "\n",
    "        result[\"qid\"] = df_query_train[\"qid\"].tolist() * top_k\n",
    "        result = result.sort_values(by=['qid']).reset_index(drop=True)\n",
    "        result[\"Q0\"] = [\"Q0\"] * len(np_result)\n",
    "        result[\"pid\"] = [df_passage.iloc[x['corpus_id']]['pid'] for x in np_result]\n",
    "        result[\"rank\"] = list(range(1, top_k+1)) * len(df_query_train)\n",
    "        result[\"score\"] = [x['score'] for x in np_result]\n",
    "        result[\"tag\"] = [tag] * len(np_result)\n",
    "\n",
    "    elif tag == \"SimCSE_cross\":\n",
    "        result['tag'] = tag\n",
    "        result['Q0'] = 'Q0'\n",
    "        result = result[[\"qid\", \"Q0\", \"pid\", \"rank\", \"score\", \"tag\"]]\n",
    "\n",
    "    if run_save:\n",
    "        run_save_path = os.path.join(data_path, f\"runs/{tag}.tsv\")\n",
    "        result.to_csv(run_save_path, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T02:08:36.951296100Z",
     "start_time": "2023-08-09T02:08:36.878219500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  SimCSE (Bi-Encoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#bi_model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "bi_model_name = \"aubmindlab/bert-large-arabertv02\" # run 0 : pAP@10 = 0.289\n",
    "#bi_model_name = \"wissamantoun/araelectra-base-artydiqa\" # run 1 : pAP@10 = 0.437\n",
    "#bi_model_name = \"salti/AraElectra-base-finetuned-ARCD\" # run 2 : pAP@10 = 0.397\n",
    "#bi_model_name = \"ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA\" # run 3 : pAP@10 = 0.435\n",
    "#bi_model_name = \"timpal0l/mdeberta-v3-base-squad2\" # run 4 : pAP@10 = 0.367\n",
    "#bi_model_name = \"gfdgdfgdg/arap_qa_bert\" # run 5 : pAP@10 = 0.184\n",
    "#bi_model_name = \"gfdgdfgdg/arap_qa_bert_large_v2\" # run6 : pAP@10 = 0.372\n",
    "#bi_model_name = \"gfdgdfgdg/arap_qa_bert_v2\" # run7 : pAP@10 = 0.344\n",
    "#bi_model_name = \"zohaib99k/Bert_Arabic-SQuADv2-QA\" # run8 : pAP@10 = 0.435\n",
    "#bi_model_name = \"arabi-elidrisi/ArabicDistilBERT_QA\" #run 9 : pAP@10 = 0.343\n",
    "#bi_model_name = \"MMars/Question_Answering_AraBERT_xtreme_ar\" #run 10 : pAP@10 = 0.337\n",
    "# bi_model_name = \"abdalrahmanshahrour/ArabicQA\" # run 11 : pAP@10 = 0.304\n",
    "# bi_model_name = \"abdalrahmanshahrour/xtremeQA-ar\" # run 12 : pAP@10 = 0.120\n",
    "\n",
    "# model_save_path = os.path.join(data_path, f'model/training_simcse-{bi_model_name}-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T01:38:09.448303900Z",
     "start_time": "2023-08-09T01:38:09.433974100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding_model Max Sequence Length: 256\n",
      "word_embedding_model dimension 1024\n",
      "pooling_model sentence embedding dimension 1024\n",
      "2023-08-08 18:38:11 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(bi_model_name, max_seq_length=256)\n",
    "print(\"word_embedding_model Max Sequence Length:\", word_embedding_model.max_seq_length)\n",
    "print(\"word_embedding_model dimension\", word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "print(\"pooling_model sentence embedding dimension\", pooling_model.get_sentence_embedding_dimension())\n",
    "\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "# bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T01:38:11.270943400Z",
     "start_time": "2023-08-09T01:38:09.449303600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_samples_passage = []\n",
    "#     add query train to train samples to make a more generalized embedding model\n",
    "for _, row in df_passage.iterrows():\n",
    "    train_samples_passage.append(InputExample(texts=[row['passage'], row['passage']]))\n",
    "train_biencoder_dataloader = DataLoader(train_samples_passage, shuffle=True, batch_size=train_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T02:19:00.689477200Z",
     "start_time": "2023-08-09T02:19:00.648468900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_samples_qp = []\n",
    "for _, row in df_qppair_train.iterrows():\n",
    "    query_id = row['qid']\n",
    "    query = df_query_train[df_query_train['qid'] == query_id]['query'].tolist()[0]\n",
    "    passage_id = row['docid']\n",
    "    if passage_id == '-1':\n",
    "        continue\n",
    "    else:\n",
    "        passage = df_passage[df_passage['pid'] == passage_id]['passage'].tolist()[0]\n",
    "        label = row['relevance']\n",
    "        #positive sample\n",
    "        train_samples_qp.append(InputExample(texts=[query, passage], label=label))\n",
    "        train_samples_qp.append(InputExample(texts=[passage, query], label=label))\n",
    "\n",
    "# add query train to train samples to make a more generalized embedding model\n",
    "train_qp_dataloader = DataLoader(train_samples_qp, shuffle=True, batch_size=train_batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train Bi-Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure the training\n",
    "num_epochs = 10\n",
    "top_k = 50\n",
    "\n",
    "# train_biencoder_loss = losses.MultipleNegativesRankingLoss(bi_encoder)\n",
    "train_biencoder_loss = losses.ContrastiveLoss(bi_encoder)\n",
    "# train_biencoder_loss = losses.CosineSimilarityLoss(bi_encoder)\n",
    "# train_biencoder_loss = losses.OnlineContrastiveLoss(bi_encoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-08 19:19:02 - Warmup-steps: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72d46faea53e4199b415897833593fd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "addeefc6ba0344e1be7a3e6ae971f1d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6862743ce41f4782aea26da3a2784e33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "097bc556f6a84bf8a9f1bed167b27f82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1a2b219e74347be91d0d4135a450557"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be1f950bc0164ca8ab8609f685561ae2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28ad3f2adcac4805842342e34fb934f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b009aa567bb04f5cb1efb82950be11c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ada45d3404bc46b5989c7b452d01097d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fcbe6ce5331420c9e95c36aab456721"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e1a1834ba87426b9599156d230bd813"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warmup_steps = math.ceil(len(train_biencoder_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# logging.info(\"Performance before training\")\n",
    "# dev_evaluator(model)\n",
    "\n",
    "bi_encoder.fit(train_objectives=[(train_biencoder_dataloader, train_biencoder_loss)],\n",
    "          # evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=100,\n",
    "          warmup_steps=warmup_steps,\n",
    "          # output_path=model_save_path\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:40:15.227791300Z",
     "start_time": "2023-08-09T02:19:02.246792100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce45dfcbac644f6c8bab53bcaa20e2e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97ffb6bce382483aba58ca378fae121b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca9f066a8331416a86865515f1f1b7d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "passage_embeddings = bi_encoder.encode(df_passage['passage'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "query_train_embeddings = bi_encoder.encode(df_query_train['query'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "query_dev_embeddings = bi_encoder.encode(df_query_dev['query'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "df_passage['embedding'] = passage_embeddings.cpu().numpy().tolist()\n",
    "df_query_train['embedding'] = query_train_embeddings.cpu().numpy().tolist()\n",
    "df_query_dev['embedding'] = query_dev_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "# for sentence, embedding in zip(df_passage, passage_embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:40:36.390986Z",
     "start_time": "2023-08-09T03:40:15.273264400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# test_samples = []\n",
    "# with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "#     reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "#     for row in reader:\n",
    "#         if row['split'] == 'test':\n",
    "#             score = float(row['score']) / 5.0 #Normalize score to range 0 ... 1\n",
    "#             test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))\n",
    "#\n",
    "# model = SentenceTransformer(model_save_path)\n",
    "# test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test')\n",
    "# test_evaluator(model, output_path=model_save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:40:36.404635200Z",
     "start_time": "2023-08-09T03:40:36.390986Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Semantic Search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aubmindlab/bert-large-arabertv02 ContrastiveLoss(\n",
      "  (model): SentenceTransformer(\n",
      "    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "    (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "    (2): Dense({'in_features': 1024, 'out_features': 256, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      "  )\n",
      ")\n",
      "Format check: Passed\n",
      "    map  recip_rank\n",
      "0.00847    0.021135\n"
     ]
    },
    {
     "data": {
      "text/plain": "      qid  Q0        pid  rank     score               tag\n0     101  Q0    112:1-4     1  0.205354  SimCSE_biencoder\n1     101  Q0   22:19-22     2  0.194922  SimCSE_biencoder\n2     101  Q0  5:116-120     3  0.193230  SimCSE_biencoder\n3     101  Q0   35:19-26     4  0.189907  SimCSE_biencoder\n4     101  Q0   56:27-40     5  0.185209  SimCSE_biencoder\n...   ...  ..        ...   ...       ...               ...\n8695  427  Q0   21:89-90    46  0.157940  SimCSE_biencoder\n8696  427  Q0   18:78-82    47  0.157008  SimCSE_biencoder\n8697  427  Q0   24:46-54    48  0.156108  SimCSE_biencoder\n8698  427  Q0  5:106-108    49  0.155555  SimCSE_biencoder\n8699  427  Q0     34:6-9    50  0.155263  SimCSE_biencoder\n\n[8700 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>Q0</th>\n      <th>pid</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>112:1-4</td>\n      <td>1</td>\n      <td>0.205354</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>22:19-22</td>\n      <td>2</td>\n      <td>0.194922</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>5:116-120</td>\n      <td>3</td>\n      <td>0.193230</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>35:19-26</td>\n      <td>4</td>\n      <td>0.189907</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>56:27-40</td>\n      <td>5</td>\n      <td>0.185209</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8695</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>21:89-90</td>\n      <td>46</td>\n      <td>0.157940</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>8696</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>18:78-82</td>\n      <td>47</td>\n      <td>0.157008</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>8697</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>24:46-54</td>\n      <td>48</td>\n      <td>0.156108</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>8698</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>5:106-108</td>\n      <td>49</td>\n      <td>0.155555</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n    <tr>\n      <th>8699</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>34:6-9</td>\n      <td>50</td>\n      <td>0.155263</td>\n      <td>SimCSE_biencoder</td>\n    </tr>\n  </tbody>\n</table>\n<p>8700 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = \"SimCSE_biencoder\"\n",
    "\n",
    "hits = util.semantic_search(query_train_embeddings, passage_embeddings, top_k=top_k)\n",
    "df_run = save_query_passage_retrieval(hits, tag, run_save=True)\n",
    "\n",
    "print(bi_model_name, train_biencoder_loss)\n",
    "! python QQA23_TaskA_eval.py \\\n",
    "    -r \"../data/runs/SimCSE_biencoder.tsv\" \\\n",
    "    -q \"../data/qrels/QQA23_TaskA_qrels_train.gold\"\n",
    "\n",
    "df_run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:50:45.271918700Z",
     "start_time": "2023-08-09T03:50:44.577780100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Re-ranker (Cross-Encoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# cross_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# cross_model_name = \"cross-encoder/ms-marco-TinyBERT-L-2\"\n",
    "cross_model_name = \"distilroberta-base\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:50:54.167056Z",
     "start_time": "2023-08-09T03:50:54.157511300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-08 20:50:55 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = CrossEncoder(cross_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:50:55.775209500Z",
     "start_time": "2023-08-09T03:50:54.560188900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train Cross-Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-08 20:51:03 - Warmup-steps: 60\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95bc4074754f48f2aa9c64a4630601e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db26748f28874b7b870242dade1f253e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bef58510321408ea6d8ebcb1b5469fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa4a824b64fb48e39f050dcbe7b60ffb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6486773e86a543f8b4b749c31a2fe97b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f38e1927b91499aa04325006d3ebdc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b35f06cb3af54b85ac2d89fb44c82250"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d24e43266f745bb8f6985f20c369646"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e0bdf303a234b2fa87c9948972c7336"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3d76a26465b47de9f428012247db07d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/60 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84531f2b30ef4aa085970b8c599f52c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tune cross-encoder on the query-passage\n",
    "train_cross_loss = losses.ContrastiveLoss(cross_encoder)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "warmup_steps = math.ceil(len(train_qp_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "cross_encoder.fit(train_qp_dataloader,\n",
    "          # evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=100,\n",
    "          warmup_steps=warmup_steps,\n",
    "          # output_path=model_save_path\n",
    "          )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:54:14.768885200Z",
     "start_time": "2023-08-09T03:51:03.015216100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-encoder model name distilroberta-base\n",
      "Format check: Passed\n",
      "     map  recip_rank\n",
      "0.005904    0.030252\n"
     ]
    },
    {
     "data": {
      "text/plain": "      qid  Q0        pid  rank     score           tag\n0     101  Q0   42:20-22     1  0.999807  SimCSE_cross\n1     101  Q0    7:54-56     2  0.999807  SimCSE_cross\n2     101  Q0  4:163-166     3  0.999807  SimCSE_cross\n3     101  Q0   42:27-31     4  0.999807  SimCSE_cross\n4     101  Q0     42:1-6     5  0.999807  SimCSE_cross\n...   ...  ..        ...   ...       ...           ...\n1735  427  Q0     49:1-5     6  0.999806  SimCSE_cross\n1736  427  Q0   66:10-12     7  0.999806  SimCSE_cross\n1737  427  Q0   43:74-80     8  0.999806  SimCSE_cross\n1738  427  Q0    7:44-45     9  0.999806  SimCSE_cross\n1739  427  Q0  2:204-207    10  0.999806  SimCSE_cross\n\n[1740 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>Q0</th>\n      <th>pid</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>42:20-22</td>\n      <td>1</td>\n      <td>0.999807</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>7:54-56</td>\n      <td>2</td>\n      <td>0.999807</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>4:163-166</td>\n      <td>3</td>\n      <td>0.999807</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>42:27-31</td>\n      <td>4</td>\n      <td>0.999807</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>42:1-6</td>\n      <td>5</td>\n      <td>0.999807</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1735</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>49:1-5</td>\n      <td>6</td>\n      <td>0.999806</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>1736</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>66:10-12</td>\n      <td>7</td>\n      <td>0.999806</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>1737</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>43:74-80</td>\n      <td>8</td>\n      <td>0.999806</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>1738</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>7:44-45</td>\n      <td>9</td>\n      <td>0.999806</td>\n      <td>SimCSE_cross</td>\n    </tr>\n    <tr>\n      <th>1739</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>2:204-207</td>\n      <td>10</td>\n      <td>0.999806</td>\n      <td>SimCSE_cross</td>\n    </tr>\n  </tbody>\n</table>\n<p>1740 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = \"SimCSE_cross\"\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for qid, filter_passages in df_run.groupby('qid'):\n",
    "    cross_inp = []\n",
    "    q = df_query_train[df_query_train['qid'] == qid]\n",
    "    for x in filter_passages.values:\n",
    "        p = df_passage[df_passage['pid'] == x[2]]\n",
    "        cross_inp.append([q['query'].tolist()[0], p['passage'].tolist()[0]])\n",
    "\n",
    "    cross_scores = cross_encoder.predict(cross_inp, show_progress_bar=False)\n",
    "    df_run = pd.DataFrame({'qid': qid, 'pid': filter_passages['pid'].values, 'score': cross_scores})\n",
    "    df_run = df_run.sort_values(by=['score'], ascending=False)[:10]\n",
    "    df_run['rank'] = range(1, len(df_run) + 1)\n",
    "\n",
    "    df_final = pd.concat([df_final, df_run], ignore_index=True)\n",
    "\n",
    "df_final = save_query_passage_retrieval(df_final, tag, run_save=True)\n",
    "\n",
    "print(\"cross-encoder model name\", cross_model_name)\n",
    "! python QQA23_TaskA_eval.py \\\n",
    "    -r \"../data/runs/SimCSE_cross.tsv\" \\\n",
    "    -q \"../data/qrels/QQA23_TaskA_qrels_train.gold\"\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T03:54:44.188163700Z",
     "start_time": "2023-08-09T03:54:14.773300400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format check: Passed\n",
      "    map  recip_rank\n",
      "0.01003    0.045361\n"
     ]
    }
   ],
   "source": [
    "! python QQA23_TaskA_eval.py \\\n",
    "    -r \"../data/runs/SimCSE_cross.tsv\" \\\n",
    "    -q \"../data/qrels/QQA23_TaskA_qrels_train.gold\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T02:17:21.269891700Z",
     "start_time": "2023-08-09T02:17:20.753503500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BM25 - Search\n",
    "Search in the index and find the relevant passages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# initialize the BM25 retrieval model\n",
    "BM25_model = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"}, num_results=top_k)\n",
    "# wmodel=\"TF_IDF\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T00:15:59.991594700Z",
     "start_time": "2023-08-09T00:15:59.957087800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "     question-id  Q0  passage-id  rank      score   tag\n0            101  Q0     7:85-93     0  14.236550  BM25\n1            101  Q0    29:36-37     1  13.002848  BM25\n2            101  Q0    11:89-95     2  12.651979  BM25\n3            101  Q0    11:84-88     3  11.658893  BM25\n4            101  Q0  26:176-191     4   6.080667  BM25\n...          ...  ..         ...   ...        ...   ...\n6503         427  Q0   3:199-200    45   6.774431  BM25\n6504         427  Q0     6:19-21    46   6.773260  BM25\n6505         427  Q0     5:15-17    47   6.742466  BM25\n6506         427  Q0      15:1-9    48   6.715582  BM25\n6507         427  Q0     5:59-60    49   6.704180  BM25\n\n[6508 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question-id</th>\n      <th>Q0</th>\n      <th>passage-id</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>7:85-93</td>\n      <td>0</td>\n      <td>14.236550</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>29:36-37</td>\n      <td>1</td>\n      <td>13.002848</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>11:89-95</td>\n      <td>2</td>\n      <td>12.651979</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>11:84-88</td>\n      <td>3</td>\n      <td>11.658893</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>101</td>\n      <td>Q0</td>\n      <td>26:176-191</td>\n      <td>4</td>\n      <td>6.080667</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6503</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>3:199-200</td>\n      <td>45</td>\n      <td>6.774431</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>6504</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>6:19-21</td>\n      <td>46</td>\n      <td>6.773260</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>6505</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>5:15-17</td>\n      <td>47</td>\n      <td>6.742466</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>6506</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>15:1-9</td>\n      <td>48</td>\n      <td>6.715582</td>\n      <td>BM25</td>\n    </tr>\n    <tr>\n      <th>6507</th>\n      <td>427</td>\n      <td>Q0</td>\n      <td>5:59-60</td>\n      <td>49</td>\n      <td>6.704180</td>\n      <td>BM25</td>\n    </tr>\n  </tbody>\n</table>\n<p>6508 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = \"BM25\"\n",
    "\n",
    "# search using BM25 model\n",
    "df_run = BM25_model.transform(df_query_train)\n",
    "\n",
    "# save the run in trec format to a file\n",
    "df_run = save_query_passage_retrieval(df_run, tag, run_save=True)\n",
    "\n",
    "df_run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T00:16:00.393942800Z",
     "start_time": "2023-08-09T00:15:59.985595600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "  qid  docid     docno  rank     score  query\n0   1   1115  66:10-12     0  8.576149  عمران\n1   1    116   3:33-41     1  8.129249  عمران",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>docid</th>\n      <th>docno</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1115</td>\n      <td>66:10-12</td>\n      <td>0</td>\n      <td>8.576149</td>\n      <td>عمران</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>116</td>\n      <td>3:33-41</td>\n      <td>1</td>\n      <td>8.129249</td>\n      <td>عمران</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"عمران\"\n",
    "BM25_model.search(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T00:16:00.422465500Z",
     "start_time": "2023-08-09T00:16:00.394942800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_emb(l1, l2):\n",
    "    l2 = sentence_embedding.encode(l2)\n",
    "    l1.extend(l2)\n",
    "    return l1\n",
    "\n",
    "def get_embedding(text, n=50):\n",
    "    batched_num = [text[i:i + n] for i in range(0, len(text), n)]\n",
    "    texts_embed = reduce(batch_emb, batched_num, [])\n",
    "    print(len(texts_embed))\n",
    "    return texts_embed\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RocketQA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T00:16:00.763070300Z",
     "start_time": "2023-08-09T00:16:00.409949200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format check: Passed\n",
      "     map  recip_rank\n",
      "0.170291    0.313333\n"
     ]
    }
   ],
   "source": [
    "! python QQA23_TaskA_eval.py \\\n",
    "    -r \"../data/runs/bigIR_BM25.tsv\" \\\n",
    "    -q \"../data/qrels/QQA23_TaskA_qrels_dev.gold\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are just evaluating the perfect run for the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python QQA23_TaskA_eval.py \\\n",
    "    -r \"../data/runs/dev_perfect.tsv\" \\\n",
    "    -q \"../data/qrels/QQA23_TaskA_qrels_dev.gold\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

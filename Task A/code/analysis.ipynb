{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:16.856280800Z",
     "start_time": "2023-08-15T20:27:16.848279900Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import arabicstopwords.arabicstopwords as ar_stp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import torch\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, CrossEncoder, util, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "from snowballstemmer import stemmer\n",
    "from torch import nn\n",
    "# from simcse import SimSCE\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:17.168445300Z",
     "start_time": "2023-08-15T20:27:17.161430400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T08:04:53.469544200Z",
     "start_time": "2023-08-11T08:04:53.454643500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "index_path = os.path.join(data_path, \"QPC_Index/data.properties\")\n",
    "\n",
    "query_train_path = os.path.join(data_path, \"QQA23_TaskA_train.tsv\")\n",
    "query_dev_path = os.path.join(data_path, \"QQA23_TaskA_dev.tsv\")\n",
    "\n",
    "passage_path = os.path.join(data_path, \"Thematic_QPC/QQA23_TaskA_QPC_v1.1.tsv\")\n",
    "\n",
    "qp_pair_train_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_train.gold\")\n",
    "qp_pair_dev_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_dev.gold\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:17.576986600Z",
     "start_time": "2023-08-15T20:27:17.569481500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:35.422287500Z",
     "start_time": "2023-08-11T05:37:35.406190400Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# read file based on its extension (tsv or xlsx)\n",
    "def read_file(input_file, sep=\"\\t\", names = \"\"):\n",
    "    if input_file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(input_file)\n",
    "    else:\n",
    "        if names != \"\":\n",
    "            df = pd.read_csv(input_file, sep=sep, names=names,encoding=\"utf-8\")\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, sep=sep,encoding=\"utf-8\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:18.085341Z",
     "start_time": "2023-08-15T20:27:18.081341800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "qrels_columns = [\"qid\", \"Q0\", \"docid\", \"relevance\"]\n",
    "\n",
    "def read_qrels_file(qrels_file):\n",
    "    # split_token = '\\t' if format_checker.is_tab_sparated(qrels_file) else  \"\\s+\"\n",
    "    df_qrels = pd.read_csv(qrels_file, sep='\\t', names=qrels_columns)\n",
    "    df_qrels[\"qid\"] = df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docid\"] = df_qrels[\"docid\"].astype(str)\n",
    "    return df_qrels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:18.616002Z",
     "start_time": "2023-08-15T20:27:18.599497300Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:37.652006400Z",
     "start_time": "2023-08-11T05:37:37.639499500Z"
    }
   },
   "source": [
    "## Cleaning & Preprocessing\n",
    "Clean text from urls, handles, special characters, tabs, line jumps, extra white space, and puntuations.\n",
    "Preprocess the arabic input text by performing normalization, stemming, and removing stop words."
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:20.318333Z",
     "start_time": "2023-08-15T20:27:20.304753700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean text from urls, handles, special characters, tabs, line jumps, and extra white space.\n",
    "def clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "    text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "    text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
    "    text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Removing punctuations in string using regex\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:21.685681600Z",
     "start_time": "2023-08-15T20:27:21.669236700Z"
    }
   },
   "outputs": [],
   "source": [
    "# arabic stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "\n",
    "# remove arabic stop words\n",
    "def ar_remove_stop_words(sentence):\n",
    "    terms=[]\n",
    "    stopWords= set(ar_stp.stopwords_list())\n",
    "    for term in sentence.split() : \n",
    "        if term not in stopWords :\n",
    "            terms.append(term)\n",
    "    return \" \".join(terms)\n",
    "\n",
    "\n",
    "# normalize the arabic text\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return(text)\n",
    "\n",
    "# stem the arabic text\n",
    "def ar_stem(sentence):\n",
    "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
    "\n",
    "\n",
    "# apply all preprocessing steps needed for Arabic text\n",
    "def preprocess_arabic(text): \n",
    "    text = normalize_arabic(text)\n",
    "    text = ar_remove_stop_words(text)\n",
    "    text = ar_stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def prepare_data(path, column, id_type, id_column='docno'):\n",
    "        df = read_file(path, names=['docno', 'text'])\n",
    "\n",
    "        print(\"Cleaning passages\")\n",
    "        # apply the cleaning functions on the queries/questions\n",
    "        df[column] = df['text'].apply(clean)\n",
    "\n",
    "        # apply normalization, stemming and stop word removal\n",
    "        print(\"Preprocessing - Applying normalization, stemming and stop word removal\")\n",
    "        df[column] = df[column].apply(preprocess_arabic)\n",
    "\n",
    "        df[id_type] = df[id_column].astype(str) # convert the id column to string\n",
    "        df = df[[id_type, 'text', column]] # keep the columns needed for search\n",
    "\n",
    "        print(\"Done with preparation!\")\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:27:34.766187900Z",
     "start_time": "2023-08-15T20:27:34.759677500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:39.775198Z",
     "start_time": "2023-08-11T05:37:39.759112600Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n",
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n",
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n"
     ]
    }
   ],
   "source": [
    "df_passage = prepare_data(passage_path, 'passage', 'pid')\n",
    "\n",
    "df_query_train = prepare_data(query_train_path, 'query', 'qid')\n",
    "df_query_dev = prepare_data(query_dev_path, 'query', 'qid')\n",
    "\n",
    "df_qppair_train = read_qrels_file(qp_pair_train_path)\n",
    "\n",
    "df_qppair_dev = read_qrels_file(qp_pair_dev_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:28:12.849483600Z",
     "start_time": "2023-08-15T20:28:09.343863700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def stats(df):\n",
    "    all = df.tolist()\n",
    "    splitted = [p.split() for p in all]\n",
    "    len_splitted = [len(p) for p in splitted]\n",
    "    print(max(len_splitted), min(len_splitted), mean(len_splitted))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:28:46.596773Z",
     "start_time": "2023-08-15T20:28:46.581137900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load models and save the tsv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def save_query_passage_retrieval(result, tag, run_save=False, df_query= df_query_train, top_k=10):\n",
    "    if tag == \"BM25\":\n",
    "        result[\"Q0\"] = [\"Q0\"] * len(result)\n",
    "        result[\"tag\"] = [tag] * len(result)\n",
    "        result['qid'] = result[\"qid\"]\n",
    "        result['pid'] = result[\"docno\"]\n",
    "        tag = \"BM25_Final\"\n",
    "        result = result[[\"qid\", \"Q0\", \"pid\", \"rank\", \"score\", \"tag\"]]\n",
    "\n",
    "    elif tag == \"SimCSE_biencoder\":\n",
    "        np_result = np.array(result).flatten()\n",
    "        result = pd.DataFrame()\n",
    "\n",
    "        result[\"qid\"] = df_query[\"qid\"].tolist() * top_k\n",
    "        result = result.sort_values(by=['qid']).reset_index(drop=True)\n",
    "        result[\"Q0\"] = [\"Q0\"] * len(result)\n",
    "        result[\"pid\"] = [df_passage.iloc[x['corpus_id']]['pid'] for x in np_result]\n",
    "        result[\"rank\"] = list(range(1, top_k+1)) * len(df_query)\n",
    "        result[\"score\"] = [x['score'] for x in np_result]\n",
    "        result[\"tag\"] = [tag] * len(np_result)\n",
    "\n",
    "    elif tag == \"SimCSE_bmbiencd\":\n",
    "        df_result = pd.DataFrame()\n",
    "        for i in range(len(bm25_biencoder_hit)):\n",
    "            for j in range(len(bm25_biencoder_hit[i])):\n",
    "                new_record = pd.DataFrame([{\"qid\": df_query_dev['qid'].tolist()[i],\n",
    "                        \"Q0\": \"Q0\",\n",
    "                        \"pid\": bm25_biencoder_hit[i][j]['corpus_id'],\n",
    "                        \"rank\": j,\n",
    "                        \"score\": bm25_biencoder_hit[i][j]['score'],\n",
    "                        \"tag\": tag\n",
    "                    }])\n",
    "                df_result = pd.concat([df_result, new_record], ignore_index=True)\n",
    "        result = df_result\n",
    "\n",
    "    elif tag == \"SimCSE_cross\":\n",
    "        result['tag'] = tag\n",
    "        result['Q0'] = 'Q0'\n",
    "        result = result[[\"qid\", \"Q0\", \"pid\", \"rank\", \"score\", \"tag\"]]\n",
    "\n",
    "    if run_save:\n",
    "        run_save_path = os.path.join(data_path, f\"runs/{tag}.tsv\")\n",
    "        result.to_csv(run_save_path, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:28:50.456343800Z",
     "start_time": "2023-08-15T20:28:50.440322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "model_path = \"../data/model\"\n",
    "fine_tuned_path = \"../data/fine_tune\"\n",
    "sbert_path = \"../data/sentence_transformers\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:28:52.612142500Z",
     "start_time": "2023-08-15T20:28:52.598591700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fine_tune\\ct-model\n"
     ]
    }
   ],
   "source": [
    "for path in [fine_tuned_path, sbert_path]:\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for model_dir in dirs:\n",
    "            m_path = os.path.join(root, model_dir)\n",
    "            print(m_path)\n",
    "\n",
    "            model = SentenceTransformer(m_path)\n",
    "\n",
    "            passage_embeddings = model.encode(df_passage['passage'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "            query_train_embeddings = model.encode(df_query_train['query'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "            query_dev_embeddings = model.encode(df_query_dev['query'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "            # df_passage['embedding'] = passage_embeddings.cpu().numpy().tolist()\n",
    "            # df_query_train['embedding'] = query_train_embeddings.cpu().numpy().tolist()\n",
    "            # df_query_dev['embedding'] = query_dev_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T20:16:10.050222600Z",
     "start_time": "2023-08-15T20:16:09.117691800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Bi-Encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding_model Max Sequence Length: 256\n",
      "word_embedding_model dimension 768\n",
      "pooling_model sentence embedding dimension 768\n",
      "2023-08-14 01:32:45 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "#TODO : change max_seq_length to 384 or 512\n",
    "bi_model_name = \"aubmindlab/bert-base-arabert\"\n",
    "\n",
    "max_seq_length = 256\n",
    "word_embedding_model = models.Transformer(bi_model_name, max_seq_length=max_seq_length)\n",
    "print(\"word_embedding_model Max Sequence Length:\", word_embedding_model.max_seq_length)\n",
    "print(\"word_embedding_model dimension\", word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "print(\"pooling_model sentence embedding dimension\", pooling_model.get_sentence_embedding_dimension())\n",
    "\n",
    "#TODO : change out_features to 512\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=max_seq_length, activation_function=nn.Tanh())\n",
    "\n",
    "# bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:45.782037700Z",
     "start_time": "2023-08-14T08:32:44.608716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    " # define some global constants\n",
    "TEXT = \"text\"\n",
    "QUERY = \"query\"\n",
    "LABEL = \"label\"\n",
    "RANK = \"rank\"\n",
    "TAG = \"tag\"\n",
    "SCORE = \"score\"\n",
    "QID = \"qid\"\n",
    "DOC_NO = \"docno\"\n",
    "DOCID = \"docid\"\n",
    "\n",
    "def prepare_query_for_search(query_path, query_column=TEXT,\n",
    "                        id_column=DOC_NO):\n",
    "\n",
    "        names = [DOC_NO, TEXT]\n",
    "        print(\"Cleaning queries and applying preprocessing steps\")\n",
    "        df_query = read_file(query_path, names=names)\n",
    "        # apply the cleaning functions on the queries/questions\n",
    "        df_query[QUERY] =df_query[query_column].apply(clean)\n",
    "\n",
    "        # apply normalization, stemming and stop word removal\n",
    "        print(\"Applying normalization, stemming and stop word removal\")\n",
    "        df_query[QUERY] =df_query[QUERY].apply(preprocess_arabic)\n",
    "\n",
    "        df_query[QID] = df_query[id_column].astype(str) # convert the id column to string\n",
    "        df_query = df_query[[QID, QUERY]] # keep the columns needed for search\n",
    "        print(\"Done with preparation!\")\n",
    "        return df_query"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T09:12:30.233231200Z",
     "start_time": "2023-08-14T09:12:30.225224400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n"
     ]
    }
   ],
   "source": [
    "BM25_model = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"}, num_results=1000)\n",
    "\n",
    "# 2. read the query file and prepare it for search to match pyterrier format\n",
    "df_query = prepare_query_for_search(query_train_path)\n",
    "\n",
    "# 3. search using BM25 model\n",
    "df_run = BM25_model.transform(df_query)\n",
    "\n",
    "# 4. save the run in trec format to a file\n",
    "df_run[\"Q0\"] = [\"Q0\"] * len(df_run)\n",
    "df_run[\"tag\"] = [\"BM25\"] * len(df_run)\n",
    "df_run['question-id'] = df_run[\"qid\"]\n",
    "df_run['passage-id'] = df_run[\"docno\"]\n",
    "df_run = df_run[[\"question-id\", \"Q0\", \"passage-id\", \"rank\", \"score\", \"tag\"]]\n",
    "df_run.to_csv(\"../data/runs/GYM_BM25.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "# df_run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:20.217744400Z",
     "start_time": "2023-08-14T12:42:19.090018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# ! python QQA23_TaskA_eval.py \\\n",
    "#     -r \"../data/runs/GYM_BM25.tsv\" \\\n",
    "#     -q \"../data/qrels/QQA23_TaskA_qrels_dev.gold\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:21.289999400Z",
     "start_time": "2023-08-14T12:42:21.267794200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "14 17\n",
      "0 1\n",
      "0 2\n",
      "1 1\n",
      "1 1\n",
      "5 7\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "5 5\n",
      "2 2\n",
      "0 3\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 3\n",
      "1 1\n",
      "0 3\n",
      "1 2\n",
      "1 1\n",
      "1 3\n",
      "0 2\n",
      "0 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "2 3\n",
      "1 1\n",
      "2 4\n",
      "2 2\n",
      "1 3\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "8 8\n",
      "1 1\n",
      "0 2\n",
      "4 4\n",
      "3 7\n",
      "5 5\n",
      "1 2\n",
      "13 26\n",
      "3 7\n",
      "1 1\n",
      "4 5\n",
      "0 1\n",
      "2 2\n",
      "1 1\n",
      "9 10\n",
      "3 3\n",
      "8 9\n",
      "1 1\n",
      "6 7\n",
      "4 4\n",
      "4 4\n",
      "2 2\n",
      "0 1\n",
      "8 74\n",
      "1 21\n",
      "1 1\n",
      "1 2\n",
      "4 5\n",
      "1 1\n",
      "7 7\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "3 3\n",
      "3 3\n",
      "0 1\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "2 4\n",
      "0 1\n",
      "1 3\n",
      "0 10\n",
      "15 16\n",
      "1 8\n",
      "4 4\n",
      "1 1\n",
      "3 3\n",
      "1 21\n",
      "3 8\n",
      "8 24\n",
      "2 30\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "3 3\n",
      "3 8\n",
      "3 5\n",
      "3 5\n",
      "5 6\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 5\n",
      "3 3\n",
      "0 1\n",
      "2 6\n",
      "6 7\n",
      "1 4\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 2\n",
      "0 12\n",
      "9 9\n",
      "0 1\n",
      "3 3\n",
      "7 9\n",
      "1 1\n",
      "7 9\n",
      "0 2\n",
      "0 6\n",
      "0 3\n",
      "1 1\n",
      "4 8\n",
      "2 2\n",
      "111 144\n",
      "2 2\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "20 28\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "0 1\n",
      "4 5\n",
      "0 1\n",
      "0 1\n",
      "1 8\n",
      "0 1\n",
      "1 1\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "1 20\n",
      "5 5\n",
      "2 2\n",
      "1 3\n",
      "0 1\n",
      "7 8\n",
      "3 5\n",
      "0 4\n",
      "1 3\n",
      "0 4\n",
      "1 1\n",
      "3 3\n",
      "1 25\n",
      "0 26\n",
      "6 10\n",
      "0 7\n",
      "0 1\n",
      "0 9\n",
      "2 5\n",
      "3 5\n",
      "0 1\n",
      "2 2\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "GOld_label_train = df_qppair_train.groupby('qid').apply(lambda x: x['docid'].tolist())\n",
    "GOld_label_dev = df_qppair_dev.groupby('qid').apply(lambda x: x['docid'].tolist())\n",
    "acuracy_list = []\n",
    "for qid, predicted in df_run.groupby('question-id'):\n",
    "    predicted = predicted['passage-id'].tolist()\n",
    "    actual = GOld_label_train[qid]\n",
    "    # print(qid, predicted, actual)\n",
    "    acuracy_list.append(len(set(predicted) & set(actual)) / len(set(actual)))\n",
    "    print(len(set(predicted) & set(actual)) , len(set(actual)))\n",
    "\n",
    "# print(\"Accuracy =\", sum(acuracy_list) / len(acuracy_list))\n",
    "# print(acuracy_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:22.184014Z",
     "start_time": "2023-08-14T12:42:22.156640200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

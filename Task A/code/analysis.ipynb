{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:33.851240100Z",
     "start_time": "2023-08-14T08:10:33.844234400Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import arabicstopwords.arabicstopwords as ar_stp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import torch\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, CrossEncoder, util, InputExample\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "from snowballstemmer import stemmer\n",
    "from torch import nn\n",
    "# from simcse import SimSCE\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:34.390772700Z",
     "start_time": "2023-08-14T08:10:34.371599100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T08:04:53.469544200Z",
     "start_time": "2023-08-11T08:04:53.454643500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "index_path = os.path.join(data_path, \"QPC_Index/data.properties\")\n",
    "\n",
    "query_train_path = os.path.join(data_path, \"QQA23_TaskA_train.tsv\")\n",
    "query_dev_path = os.path.join(data_path, \"QQA23_TaskA_dev.tsv\")\n",
    "\n",
    "passage_path = os.path.join(data_path, \"Thematic_QPC/QQA23_TaskA_QPC_v1.1.tsv\")\n",
    "\n",
    "qp_pair_train_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_train.gold\")\n",
    "qp_pair_dev_path = os.path.join(data_path, \"qrels\\QQA23_TaskA_qrels_dev.gold\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:36.199097400Z",
     "start_time": "2023-08-14T08:10:36.182980200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:35.422287500Z",
     "start_time": "2023-08-11T05:37:35.406190400Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# read file based on its extension (tsv or xlsx)\n",
    "def read_file(input_file, sep=\"\\t\", names = \"\"):\n",
    "    if input_file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(input_file)\n",
    "    else:\n",
    "        if names != \"\":\n",
    "            df = pd.read_csv(input_file, sep=sep, names=names,encoding=\"utf-8\")\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, sep=sep,encoding=\"utf-8\")\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:40.248418700Z",
     "start_time": "2023-08-14T08:10:40.236910100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "qrels_columns = [\"qid\", \"Q0\", \"docid\", \"relevance\"]\n",
    "\n",
    "def read_qrels_file(qrels_file):\n",
    "    # split_token = '\\t' if format_checker.is_tab_sparated(qrels_file) else  \"\\s+\"\n",
    "    df_qrels = pd.read_csv(qrels_file, sep='\\t', names=qrels_columns)\n",
    "    df_qrels[\"qid\"] = df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docid\"] = df_qrels[\"docid\"].astype(str)\n",
    "    return df_qrels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:41.093806800Z",
     "start_time": "2023-08-14T08:10:41.086297800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def load_index(index_path):\n",
    "    if not pt.started():\n",
    "        pt.init(helper_version=\"0.0.6\")\n",
    "\n",
    "    try:\n",
    "        index = pt.IndexFactory.of(index_path)\n",
    "        print(\"Index was loaded successfully from this path: \", index_path)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print('Cannot load the index, check exception details {}'.format(e))\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:41.662578400Z",
     "start_time": "2023-08-14T08:10:41.635070400Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:37.652006400Z",
     "start_time": "2023-08-11T05:37:37.639499500Z"
    }
   },
   "source": [
    "## Cleaning & Preprocessing\n",
    "Clean text from urls, handles, special characters, tabs, line jumps, extra white space, and puntuations.\n",
    "Preprocess the arabic input text by performing normalization, stemming, and removing stop words."
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:44.293313600Z",
     "start_time": "2023-08-14T08:10:44.286801900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean text from urls, handles, special characters, tabs, line jumps, and extra white space.\n",
    "def clean(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "    text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "    text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
    "    text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "    text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Removing punctuations in string using regex\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:44.757467600Z",
     "start_time": "2023-08-14T08:10:44.745465Z"
    }
   },
   "outputs": [],
   "source": [
    "# arabic stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "\n",
    "# remove arabic stop words\n",
    "def ar_remove_stop_words(sentence):\n",
    "    terms=[]\n",
    "    stopWords= set(ar_stp.stopwords_list())\n",
    "    for term in sentence.split() : \n",
    "        if term not in stopWords :\n",
    "            terms.append(term)\n",
    "    return \" \".join(terms)\n",
    "\n",
    "\n",
    "# normalize the arabic text\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    return(text)\n",
    "\n",
    "# stem the arabic text\n",
    "def ar_stem(sentence):\n",
    "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
    "\n",
    "\n",
    "# apply all preprocessing steps needed for Arabic text\n",
    "def preprocess_arabic(text): \n",
    "    text = normalize_arabic(text)\n",
    "    text = ar_remove_stop_words(text)\n",
    "    text = ar_stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def prepare_data(path, column, id_type, id_column='docno'):\n",
    "        df = read_file(path, names=['docno', 'text'])\n",
    "\n",
    "        print(\"Cleaning passages\")\n",
    "        # apply the cleaning functions on the queries/questions\n",
    "        df[column] = df['text'].apply(clean)\n",
    "\n",
    "        # apply normalization, stemming and stop word removal\n",
    "        print(\"Preprocessing - Applying normalization, stemming and stop word removal\")\n",
    "        df[column] = df[column].apply(preprocess_arabic)\n",
    "\n",
    "        df[id_type] = df[id_column].astype(str) # convert the id column to string\n",
    "        df = df[[id_type, 'text', column]] # keep the columns needed for search\n",
    "\n",
    "        print(\"Done with preparation!\")\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:45.004269400Z",
     "start_time": "2023-08-14T08:10:44.991265Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:39.775198Z",
     "start_time": "2023-08-11T05:37:39.759112600Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = load_index(index_path=index_path)\n",
    "\n",
    "# print(index.getCollectionStatistics().toString())\n",
    "# print(index.getMetaIndex().getKeys())\n",
    "\n",
    "# for kv in index.getLexicon():\n",
    "#     print((kv.getKey())+\"\\t\"+ kv.getValue().toString())\n",
    "# index.getLexicon()[\"فاعل\"].toString()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n",
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n",
      "Cleaning passages\n",
      "Preprocessing - Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n"
     ]
    }
   ],
   "source": [
    "df_passage = prepare_data(passage_path, 'passage', 'pid')\n",
    "\n",
    "df_query_train = prepare_data(query_train_path, 'query', 'qid')\n",
    "df_query_dev = prepare_data(query_dev_path, 'query', 'qid')\n",
    "\n",
    "df_qppair_train = read_qrels_file(qp_pair_train_path)\n",
    "\n",
    "df_qppair_dev = read_qrels_file(qp_pair_dev_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:10:54.083545500Z",
     "start_time": "2023-08-14T08:10:50.643335700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'ان كفر سواء اانذر ام تنذر يءمن ختم الله قلوب سمع ابصار غشا عذاب عظيم'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_passage.passage[5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:15:57.205265700Z",
     "start_time": "2023-08-14T08:15:57.195263700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model - Sentence Embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:37:45.398058Z",
     "start_time": "2023-08-11T05:37:45.303203100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     qid  Q0      docid  relevance\n0    114   0   28:76-80          1\n1    124   0  2:234-237          1\n2    126   0   37:62-74          1\n3    126   0   44:40-50          1\n4    126   0   56:41-56          1\n..   ...  ..        ...        ...\n155  428   0   47:20-24          1\n156  322   0         -1          1\n157  260   0         -1          1\n158  384   0         -1          1\n159  336   0         -1          1\n\n[160 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>Q0</th>\n      <th>docid</th>\n      <th>relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>114</td>\n      <td>0</td>\n      <td>28:76-80</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>124</td>\n      <td>0</td>\n      <td>2:234-237</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>126</td>\n      <td>0</td>\n      <td>37:62-74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>126</td>\n      <td>0</td>\n      <td>44:40-50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>126</td>\n      <td>0</td>\n      <td>56:41-56</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>428</td>\n      <td>0</td>\n      <td>47:20-24</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>322</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>260</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>384</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>336</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>160 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple passage-passage pair"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_samples_passage) = 1266\n"
     ]
    }
   ],
   "source": [
    "train_samples_passage = []\n",
    "for _, row in df_passage.iterrows():\n",
    "    train_samples_passage.append(InputExample(texts=[row['passage'], row['passage']]))\n",
    "\n",
    "print(\"len(train_samples_passage) =\", len(train_samples_passage))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:33.928172900Z",
     "start_time": "2023-08-14T08:32:33.884158800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### query-passage double pair with relevance label = 1 (positive)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_samples_qp) = 1892\n"
     ]
    }
   ],
   "source": [
    "train_samples_qp = []\n",
    "for _, row in df_qppair_train.iterrows():\n",
    "    query_id = row['qid']\n",
    "    query = df_query_train[df_query_train['qid'] == query_id]['query'].tolist()[0]\n",
    "    passage_id = row['docid']\n",
    "\n",
    "    if passage_id == '-1':\n",
    "        continue\n",
    "    else:\n",
    "        passage = df_passage[df_passage['pid'] == passage_id]['passage'].tolist()[0]\n",
    "        label = row['relevance']\n",
    "        #positive sample\n",
    "        train_samples_qp.append(InputExample(texts=[query, passage], label=label))\n",
    "        train_samples_qp.append(InputExample(texts=[passage, query], label=label))\n",
    "\n",
    "print(\"len(train_samples_qp) =\", len(train_samples_qp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:35.297253800Z",
     "start_time": "2023-08-14T08:32:34.914503700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### contrastive: query-passage double pair with relevance label = 1 (positive) and not found BM25 top-k passages with relevance label = 0 (negative)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k = 75\n",
      "len(train_samples_qp_contrastive) = 19224\n"
     ]
    }
   ],
   "source": [
    "train_samples_qp_contrastive = []\n",
    "\n",
    "top_k = 75\n",
    "print(\"top_k =\", top_k)\n",
    "BM25_model = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"}, num_results=top_k)\n",
    "\n",
    "for _, row in df_qppair_train.groupby('qid'):\n",
    "    query_id = row['qid'].tolist()[0]\n",
    "    query = df_query_train[df_query_train['qid'] == query_id]['query'].tolist()[0]\n",
    "    bm25_related_passage = BM25_model.search(query)['docno'].tolist()\n",
    "    positive_passage = row['docid'].tolist()\n",
    "    negative_passage = list(set(bm25_related_passage) - set(positive_passage))\n",
    "\n",
    "    for pos_passage in positive_passage:\n",
    "        if pos_passage == '-1':\n",
    "            continue\n",
    "        else:\n",
    "            passage = df_passage[df_passage['pid'] == pos_passage]['passage'].tolist()[0]\n",
    "            label = 1\n",
    "            #positive sample\n",
    "            train_samples_qp_contrastive.append(InputExample(texts=[query, passage], label=label))\n",
    "            train_samples_qp_contrastive.append(InputExample(texts=[passage, query], label=label))\n",
    "\n",
    "    for neg_passage in negative_passage:\n",
    "        if neg_passage == '-1':\n",
    "            continue\n",
    "        else:\n",
    "            passage = df_passage[df_passage['pid'] == neg_passage]['passage'].tolist()[0]\n",
    "            label = 0\n",
    "            #positive sample\n",
    "            train_samples_qp_contrastive.append(InputExample(texts=[query, passage], label=label))\n",
    "            train_samples_qp_contrastive.append(InputExample(texts=[passage, query], label=label))\n",
    "\n",
    "print(\"len(train_samples_qp_contrastive) =\", len(train_samples_qp_contrastive))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:38.891653800Z",
     "start_time": "2023-08-14T08:32:36.071309400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### multiple negative ranking: query-passage double pair with relevance label = 1 (positive)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_samples_qp_multiple_negative_ranking) = 1892\n"
     ]
    }
   ],
   "source": [
    "train_samples_qp_multiple_negative_ranking = []\n",
    "for _, row in df_qppair_train.groupby('qid'):\n",
    "    query_id = row['qid'].tolist()[0]\n",
    "    query = df_query_train[df_query_train['qid'] == query_id]['query'].tolist()[0]\n",
    "    positive_passage = row['docid'].tolist()\n",
    "    for pos_passage in positive_passage:\n",
    "        if pos_passage == '-1':\n",
    "            continue\n",
    "        else:\n",
    "            passage = df_passage[df_passage['pid'] == pos_passage]['passage'].tolist()[0]\n",
    "            label = 1\n",
    "            #positive sample\n",
    "            train_samples_qp_multiple_negative_ranking.append(InputExample(texts=[query, passage], label=label))\n",
    "            train_samples_qp_multiple_negative_ranking.append(InputExample(texts=[passage, query], label=label))\n",
    "\n",
    "print(\"len(train_samples_qp_multiple_negative_ranking) =\", len(train_samples_qp_multiple_negative_ranking))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:39.095339600Z",
     "start_time": "2023-08-14T08:32:38.892653500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### triple: query-positive passage-negative passage; negative passage is not found in BM25 top-k passages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k = 20\n",
      "len train_samples_qp_triple = 14047\n"
     ]
    }
   ],
   "source": [
    "top_k = 20\n",
    "print(\"top_k =\", top_k)\n",
    "BM25_model = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"}, num_results=top_k)\n",
    "\n",
    "train_samples_qp_triple = []\n",
    "\n",
    "for _, row in df_qppair_train.groupby('qid'):\n",
    "    # print(row)\n",
    "    query_id = row['qid'].tolist()[0]\n",
    "    query = df_query_train[df_query_train['qid'] == query_id]['query'].tolist()[0]\n",
    "    bm25_related_passage = BM25_model.search(query)['docno'].tolist()\n",
    "    positive_passage = row['docid'].tolist()\n",
    "    negative_passage = list(set(bm25_related_passage) - set(positive_passage))\n",
    "    # print(bm25_related_passage)\n",
    "    # print(possitive_passage)\n",
    "    # print(negative_passage)\n",
    "\n",
    "    for pos_passage_id in positive_passage:\n",
    "        for neg_passage_id in negative_passage:\n",
    "            if pos_passage_id == '-1':\n",
    "                continue\n",
    "            else:\n",
    "                pos_passage = df_passage[df_passage['pid'] == pos_passage_id]['passage'].tolist()[0]\n",
    "                neg_passage = df_passage[df_passage['pid'] == neg_passage_id]['passage'].tolist()[0]\n",
    "                train_samples_qp_triple.append(InputExample(texts=[query, pos_passage, neg_passage]))\n",
    "\n",
    "print(\"len train_samples_qp_triple =\", len(train_samples_qp_triple))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:44.607714300Z",
     "start_time": "2023-08-14T08:32:39.180946100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "['قوم شعيب',\n 'قوم موس',\n 'بن كعبه',\n 'النب معروف صبر',\n 'كفل سيده مريم',\n 'معن حطمه',\n 'اخو سيد موس',\n 'معن قارعه',\n 'معن جاثيه',\n 'اسباط',\n 'ملك سبا',\n 'عقر ناقه',\n 'عقوب ربا',\n 'عقوب سارق',\n 'شجره ملعونه',\n 'النب علم الله لغه طير والحيو',\n 'ميراث الام ولد ان يكن ولد',\n 'امر الله زكري الا يكلم ناس',\n 'مده عده مطلقه',\n 'عدد اشهر حرم',\n 'شجره ياكل كفار نار',\n 'النب دخل سجن',\n 'جبل استقر سفين نوح',\n 'فتر رضاع مولود',\n 'اين كان رحل اسراء والمعراج',\n 'حكم تعدد زواج',\n 'زكري',\n 'لبث بطن حوت',\n 'عقوب قتل خطا',\n 'عقوب قتل عمد',\n 'مطفف',\n 'عدد حمل عرش',\n 'وصف حور عين',\n 'ابو سيد يوسف سلام',\n 'قارون',\n 'كفار ظهار',\n 'كفار اليم',\n 'جزاء يقول ان لله ولد',\n 'اين تقع قبل مسلم اول',\n 'يوم خلق الله كون',\n 'فضل قدر',\n 'كتاب انزل موس',\n 'كتاب انزل عيس',\n 'لغه القر',\n 'مسيح',\n 'صنع عجل الحل لبن اسراءيل',\n 'مخلوق تسبح الله',\n 'الا تتحدث موضوع وصيه',\n 'الا تتحدث موضوع وصيه سور ماءده',\n 'ابناء سيد ابراهيم سلام',\n 'حدث لقابيل هابيل',\n 'قرن',\n 'احداث متعلقه قرن',\n 'معجزا النب موس سلام',\n 'حوار',\n 'احداث متعلقه داوود سلام',\n 'نام اهل كهف',\n 'عايش سيد عيس سلام',\n 'رسل عاش مصر',\n 'لماذ الله معجز سيد صالح ناقه',\n 'عاش سيد موس مصر',\n 'اين نزل سور علق',\n 'انواع حيوان ذكر القر',\n 'اسماء مدن مذكوره القر',\n 'قصه تسع رهط',\n 'اين عاش قوم',\n 'اهل الله قوم',\n 'خرج سيد يوسف سلام سجن',\n 'اهل الله قوم ثمود',\n 'قوم الله ال قرد',\n 'النب تكلم هدهد',\n 'اين نزل توراه سيد موس',\n 'قصه اصحاب سبت',\n 'حدث لسيد يونس ان ابتلع حوت',\n 'استغرق سيد نوح سلام ناء سفينه',\n 'اين يقع جود',\n 'احداث وقع سيد موس والخضر',\n 'دام ابتلاء ايوب سلام',\n 'سبب نزول سيد ادم جنه',\n 'النب عايش طالو',\n 'شار غزو بدر',\n 'انصار مذكور القر',\n 'حارب رسول محمد سلام مكه',\n 'حجاب مراه فرض',\n 'جهاد',\n 'فرض جهاد سبيل الله',\n 'ضد فرض جهاد',\n 'يجب ذكر اسم الله ماكل والمشرب',\n 'الله يتبع حسنه بالاذ',\n 'الله حياه دنيا',\n 'شروط قبول توبه',\n 'اثر كلام طيب',\n 'يشمل احس',\n 'محسن',\n 'مصارف زكاه',\n 'تجوز صدقه',\n 'حفظ القر ظهر قلب فرض',\n 'تجويد القر فرض',\n 'البر',\n 'يشمل البر',\n 'ابرار',\n 'علام والدلاءل تشير ال موعد يوم قيامه',\n 'الا تلمح ال عذاب قبر واهوال',\n 'نفسر اوج تناقض امر خروج مراه مسلمه قتال والصل عهد رسول الله امر قاء بيو والامتناع تبرج كتبرج جاهليه اول سور احزاب ايه 33',\n 'يوجد علاق اجزاء دماغ قدرا انس',\n 'فرق عقل والدماغ',\n 'ورد القر اشار لصو تاثير ايجاب جسم انس',\n 'ضوء نور القر',\n 'منافع صحيه لصل فجر',\n 'ذكر القر اوقا نوم معين',\n 'اشارا القر ال كرو ارض',\n 'لماذ تلبس مراه مسلمه حجاب',\n 'لماذ عبد اتا الله علم سور كهف قتل غلام ان ايه 93 سور نساء قال الله تعال يقتل مءم متعمد جزاء جهنم خالد غضب الله واعد عذاب عظيم',\n 'الا ذكر شفاء',\n 'يءثم حاكم يحكم انزل الله',\n 'لفظ عام لفظ سنه القر',\n 'انواع جهاد',\n 'فضل جهاد سبيل الله',\n 'فرق فرض عين فرض كفايه',\n 'دلاءل ان اختلاف رسول والنب',\n 'استخدم لفظ مطر القر عذاب',\n 'كلم صوم القر تعن صيام اكل والشرب',\n 'لماذ يحاسب يعذب ضال يوم قيامه ان يضلل الله هاد ورد قول تعال ايه 23 ايه 36 سور زمر',\n 'اشارا القر نها الك صهيون',\n 'شفعاء',\n 'عقوب يتهم مراه زنا دليل',\n 'دلاءل ان مسيح يصلب',\n 'شروط شفاعه',\n 'سيد يوسف سلام رسول ام نبي',\n 'دلاءل ان القر تاليف سيد محمد',\n 'سيد محمد اول مسلم',\n 'يكون سيد محمد اول مسلم انعام 163 غافر 12 ان سيد ابراهيم حنيف مسلم قبل ال عمران 67',\n 'اتهم القر بان سبب دكتاتوريه اسلاميه لكون اباح سب نساء نرد',\n 'لماذ تعادل شهاد مرا واحد شهاد رجل واحد امور ماليه او متعلق امور ماليه',\n 'تجوز شهاد اربع نساء بدل رجلين امور ماليه او متعلق امور ماليه',\n 'ان الله قدر افعال لماذ يحاسب',\n 'لماذ يتوض مسلم',\n 'غايه وضوء صلاه',\n 'مقاتل داعش مثل او مفسد ارض تنظيم ارهابيه يتوضء ايض يجعل اطهار',\n 'وضوء نظافه لماذ اعاد خروج ريح',\n 'ان سءال اساس علوم لماذ نه الله مءمن طرح اسءله جاء سور ماءده ايه 101',\n 'الا يعد تكليف مءم بالثب امام عشر انفال 65 تكليف يطاق',\n 'تعبير الان خفف الله علم ان ضعف انفال 66 يعن بان الله يكن يعلم مفترض ان الله عالم غيب',\n 'دليل ان القر صالح مكان',\n 'اتهم القر بان سبب دكتاتوريه اسلاميه لكون اباح ضرب نساء حال نشوز نرد',\n 'اتهم القر بان سبب دكتاتوريه اسلاميه لكون اباح تكفير قتال كفار حت يسلم نرد',\n 'اتهم القر بان سبب دكتاتوريه اسلاميه لكون فرض جزيه مسلم نرد',\n 'لماذ حرم الله تبن',\n 'لماذ اباح القر حرم اشهر حرم',\n 'بعث النب محمد تشمل الجن انس',\n 'تحدث سيد محمد الجن',\n 'مت يحل اسلام دم شخص',\n 'اخبر القر ذره',\n 'ذكر القر ان انجيل تم تحريف',\n 'مات مسيح فعل',\n 'اشار القر ال نقص اكسج مرتفع',\n 'ذكر القر ان توراه تم تحريف',\n 'حكم يرتد دين اسلام',\n 'تحدث حيوان القر',\n 'يجوز معامل اهل كتاب بالبر والحس',\n 'يتغير اي حرف القر',\n 'لماذ يساو اسلام رجل والمر شهاده امام قاض',\n 'يجوز سب نساء واسترقاق تفعل الجه منتسبه اسلام باطل',\n 'لماذ يكتف مسلم بالقر كريم يلجا سنه ايض',\n 'اسلام حديث شريف',\n 'اسلام دين سلام',\n 'سمح اسلام بحر اعتقاد دخول ال اسلام',\n 'سمح اسلام بحر اعتقاد دخول ال اسلام اكر دين لماذ رفض دخل اسلام خروج يجعل حرا',\n 'جود قراءا مختلف للقر يعن تحريف',\n 'اماك ذكر القر كام مقدس',\n 'لماذ يتم حذف الا منسوخه القر',\n 'سيد محمد افضل انبياء',\n 'حذر القر مءمن اتخاذ اهل كتاب اولياء',\n 'با طريق حث القر مءمن مجادل اهل كتاب']"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_train['query'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T09:01:21.062119800Z",
     "start_time": "2023-08-14T09:01:21.055120700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_passage\n",
      "162 4 43.872037914691944\n",
      "\n",
      "df_query_train\n",
      "28 1 4.885057471264368\n",
      "\n",
      "df_query_dev\n",
      "18 2 4.84\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "def stats(df):\n",
    "    all = df.tolist()\n",
    "    splitted = [p.split() for p in all]\n",
    "    len_splitted = [len(p) for p in splitted]\n",
    "    print(max(len_splitted), min(len_splitted), mean(len_splitted))\n",
    "\n",
    "\n",
    "print(\"\\ndf_passage\")\n",
    "stats(df_passage.passage)\n",
    "\n",
    "print(\"\\ndf_query_train\")\n",
    "stats(df_query_train['query'])\n",
    "\n",
    "print(\"\\ndf_query_dev\")\n",
    "stats(df_query_dev['query'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T09:02:47.836551900Z",
     "start_time": "2023-08-14T09:02:47.831508200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Bi-Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T05:41:25.811358200Z",
     "start_time": "2023-08-11T05:40:51.432672100Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embedding_model Max Sequence Length: 256\n",
      "word_embedding_model dimension 768\n",
      "pooling_model sentence embedding dimension 768\n",
      "2023-08-14 01:32:45 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "#TODO : change max_seq_length to 384 or 512\n",
    "bi_model_name = \"aubmindlab/bert-base-arabert\"\n",
    "\n",
    "max_seq_length = 256\n",
    "word_embedding_model = models.Transformer(bi_model_name, max_seq_length=max_seq_length)\n",
    "print(\"word_embedding_model Max Sequence Length:\", word_embedding_model.max_seq_length)\n",
    "print(\"word_embedding_model dimension\", word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "print(\"pooling_model sentence embedding dimension\", pooling_model.get_sentence_embedding_dimension())\n",
    "\n",
    "#TODO : change out_features to 512\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=max_seq_length, activation_function=nn.Tanh())\n",
    "\n",
    "# bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T08:32:45.782037700Z",
     "start_time": "2023-08-14T08:32:44.608716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    " # define some global constants\n",
    "TEXT = \"text\"\n",
    "QUERY = \"query\"\n",
    "LABEL = \"label\"\n",
    "RANK = \"rank\"\n",
    "TAG = \"tag\"\n",
    "SCORE = \"score\"\n",
    "QID = \"qid\"\n",
    "DOC_NO = \"docno\"\n",
    "DOCID = \"docid\"\n",
    "\n",
    "def prepare_query_for_search(query_path, query_column=TEXT,\n",
    "                        id_column=DOC_NO):\n",
    "\n",
    "        names = [DOC_NO, TEXT]\n",
    "        print(\"Cleaning queries and applying preprocessing steps\")\n",
    "        df_query = read_file(query_path, names=names)\n",
    "        # apply the cleaning functions on the queries/questions\n",
    "        df_query[QUERY] =df_query[query_column].apply(clean)\n",
    "\n",
    "        # apply normalization, stemming and stop word removal\n",
    "        print(\"Applying normalization, stemming and stop word removal\")\n",
    "        df_query[QUERY] =df_query[QUERY].apply(preprocess_arabic)\n",
    "\n",
    "        df_query[QID] = df_query[id_column].astype(str) # convert the id column to string\n",
    "        df_query = df_query[[QID, QUERY]] # keep the columns needed for search\n",
    "        print(\"Done with preparation!\")\n",
    "        return df_query"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T09:12:30.233231200Z",
     "start_time": "2023-08-14T09:12:30.225224400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal\n",
      "Done with preparation!\n"
     ]
    }
   ],
   "source": [
    "BM25_model = pt.BatchRetrieve(index, controls = {\"wmodel\": \"BM25\"}, num_results=1000)\n",
    "\n",
    "# 2. read the query file and prepare it for search to match pyterrier format\n",
    "df_query = prepare_query_for_search(query_train_path)\n",
    "\n",
    "# 3. search using BM25 model\n",
    "df_run = BM25_model.transform(df_query)\n",
    "\n",
    "# 4. save the run in trec format to a file\n",
    "df_run[\"Q0\"] = [\"Q0\"] * len(df_run)\n",
    "df_run[\"tag\"] = [\"BM25\"] * len(df_run)\n",
    "df_run['question-id'] = df_run[\"qid\"]\n",
    "df_run['passage-id'] = df_run[\"docno\"]\n",
    "df_run = df_run[[\"question-id\", \"Q0\", \"passage-id\", \"rank\", \"score\", \"tag\"]]\n",
    "df_run.to_csv(\"../data/runs/GYM_BM25.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "# df_run"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:20.217744400Z",
     "start_time": "2023-08-14T12:42:19.090018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# ! python QQA23_TaskA_eval.py \\\n",
    "#     -r \"../data/runs/GYM_BM25.tsv\" \\\n",
    "#     -q \"../data/qrels/QQA23_TaskA_qrels_dev.gold\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:21.289999400Z",
     "start_time": "2023-08-14T12:42:21.267794200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "14 17\n",
      "0 1\n",
      "0 2\n",
      "1 1\n",
      "1 1\n",
      "5 7\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "5 5\n",
      "2 2\n",
      "0 3\n",
      "2 2\n",
      "1 1\n",
      "2 2\n",
      "1 3\n",
      "1 1\n",
      "0 3\n",
      "1 2\n",
      "1 1\n",
      "1 3\n",
      "0 2\n",
      "0 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "2 3\n",
      "1 1\n",
      "2 4\n",
      "2 2\n",
      "1 3\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "0 1\n",
      "8 8\n",
      "1 1\n",
      "0 2\n",
      "4 4\n",
      "3 7\n",
      "5 5\n",
      "1 2\n",
      "13 26\n",
      "3 7\n",
      "1 1\n",
      "4 5\n",
      "0 1\n",
      "2 2\n",
      "1 1\n",
      "9 10\n",
      "3 3\n",
      "8 9\n",
      "1 1\n",
      "6 7\n",
      "4 4\n",
      "4 4\n",
      "2 2\n",
      "0 1\n",
      "8 74\n",
      "1 21\n",
      "1 1\n",
      "1 2\n",
      "4 5\n",
      "1 1\n",
      "7 7\n",
      "1 2\n",
      "0 1\n",
      "0 1\n",
      "3 3\n",
      "3 3\n",
      "0 1\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "3 3\n",
      "1 1\n",
      "1 1\n",
      "2 4\n",
      "0 1\n",
      "1 3\n",
      "0 10\n",
      "15 16\n",
      "1 8\n",
      "4 4\n",
      "1 1\n",
      "3 3\n",
      "1 21\n",
      "3 8\n",
      "8 24\n",
      "2 30\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "3 3\n",
      "3 8\n",
      "3 5\n",
      "3 5\n",
      "5 6\n",
      "0 1\n",
      "0 1\n",
      "0 1\n",
      "1 5\n",
      "3 3\n",
      "0 1\n",
      "2 6\n",
      "6 7\n",
      "1 4\n",
      "1 1\n",
      "6 6\n",
      "1 1\n",
      "2 2\n",
      "0 12\n",
      "9 9\n",
      "0 1\n",
      "3 3\n",
      "7 9\n",
      "1 1\n",
      "7 9\n",
      "0 2\n",
      "0 6\n",
      "0 3\n",
      "1 1\n",
      "4 8\n",
      "2 2\n",
      "111 144\n",
      "2 2\n",
      "0 1\n",
      "1 1\n",
      "1 1\n",
      "20 28\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "0 1\n",
      "1 1\n",
      "0 1\n",
      "0 1\n",
      "4 5\n",
      "0 1\n",
      "0 1\n",
      "1 8\n",
      "0 1\n",
      "1 1\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "1 20\n",
      "5 5\n",
      "2 2\n",
      "1 3\n",
      "0 1\n",
      "7 8\n",
      "3 5\n",
      "0 4\n",
      "1 3\n",
      "0 4\n",
      "1 1\n",
      "3 3\n",
      "1 25\n",
      "0 26\n",
      "6 10\n",
      "0 7\n",
      "0 1\n",
      "0 9\n",
      "2 5\n",
      "3 5\n",
      "0 1\n",
      "2 2\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "GOld_label_train = df_qppair_train.groupby('qid').apply(lambda x: x['docid'].tolist())\n",
    "GOld_label_dev = df_qppair_dev.groupby('qid').apply(lambda x: x['docid'].tolist())\n",
    "acuracy_list = []\n",
    "for qid, predicted in df_run.groupby('question-id'):\n",
    "    predicted = predicted['passage-id'].tolist()\n",
    "    actual = GOld_label_train[qid]\n",
    "    # print(qid, predicted, actual)\n",
    "    acuracy_list.append(len(set(predicted) & set(actual)) / len(set(actual)))\n",
    "    print(len(set(predicted) & set(actual)) , len(set(actual)))\n",
    "\n",
    "# print(\"Accuracy =\", sum(acuracy_list) / len(acuracy_list))\n",
    "# print(acuracy_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T12:42:22.184014Z",
     "start_time": "2023-08-14T12:42:22.156640200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
